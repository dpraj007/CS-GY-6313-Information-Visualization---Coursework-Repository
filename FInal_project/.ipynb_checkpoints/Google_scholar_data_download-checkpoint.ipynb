{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEOISouQFR7X",
    "outputId": "4abbf36f-f486-4e3b-b1f0-c5fc0640841f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped: Martin Farach-Colton\n",
      "Scraped: Boris Aronov\n",
      "Scraped: Juan P Bello\n",
      "Scraped: Aaron Bernstein\n",
      "Scraped: Emily Black\n",
      "Scraped: Justin Cappos\n",
      "Scraped: Yi-Jen Chiang\n",
      "Scraped: Rumi Chunara\n",
      "Scraped: Peter DePasquale\n",
      "Scraped: Ratan Dey\n",
      "Scraped: Brendan Dolan-Gavitt\n",
      "Scraped: Juliana Freire\n",
      "Scraped: Rachel Greenstadt\n",
      "Scraped: Chinmay Hegde\n",
      "Scraped: Lisa Hellerstein\n",
      "Scraped: Robert Krueger\n",
      "Scraped: Damon McCoy\n",
      "Scraped: Nasir Memon\n",
      "Scraped: Christopher Musco\n",
      "Scraped: Darryl Reeves\n",
      "Scraped: Gustavo Sandoval\n",
      "Scraped: Claudio T. Silva\n",
      "Scraped: John Sterling\n",
      "Scraped: Julia Stoyanovich\n",
      "Scraped: Torsten Suel\n",
      "Scraped: Qi Sun\n",
      "Scraped: Julian Togelius\n",
      "Scraped: Paul Torrens\n",
      "Scraped: Erdem Varol\n",
      "Scraped: Edward K Wong\n",
      "Scraped: Jay Chen\n",
      "Scraped: Paweł Korus\n",
      "Scraped: Niall L. Williams\n",
      "Scraped: Dishita G Turakhia\n",
      "Scraped: Nairen Cao\n",
      "Scraped: Eugene Vinitsky\n",
      "Scraped: Brandon Reagen\n",
      "Scraped: Oded Nov\n",
      "Scraped: Daniel B. Neill\n",
      "Scraped: Ramesh Karri  కర్రి రమేష్\n",
      "Scraped: Danny Yuxing Huang\n",
      "Scraped: Siddharth Garg\n",
      "Scraped: Chen Feng (冯晨)\n",
      "Scraped: Semiha Ergan\n",
      "Scraping completed. Results saved to 'scraped_scholar_profiles.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Read the CSV file with Google Scholar links\n",
    "df = pd.read_csv('/content/NYU_tandon_professors_links.csv', header=None, names=['url'])\n",
    "\n",
    "# Function to scrape profile data\n",
    "def scrape_profile(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    name = soup.find('div', id='gsc_prf_in').text if soup.find('div', id='gsc_prf_in') else 'N/A'\n",
    "    affiliation = soup.find('div', class_='gsc_prf_il').text if soup.find('div', class_='gsc_prf_il') else 'N/A'\n",
    "\n",
    "    citations = soup.find('td', class_='gsc_rsb_std')\n",
    "    total_citations = citations.text if citations else 'N/A'\n",
    "\n",
    "    h_index = soup.find_all('td', class_='gsc_rsb_std')[2]\n",
    "    h_index = h_index.text if h_index else 'N/A'\n",
    "\n",
    "    return {\n",
    "        'name': name,\n",
    "        'affiliation': affiliation,\n",
    "        'total_citations': total_citations,\n",
    "        'h_index': h_index,\n",
    "        'url': url\n",
    "    }\n",
    "\n",
    "# List to store scraped data\n",
    "scraped_data = []\n",
    "\n",
    "# Scrape each profile\n",
    "for url in df['url']:\n",
    "    try:\n",
    "        profile_data = scrape_profile(url)\n",
    "        scraped_data.append(profile_data)\n",
    "        print(f\"Scraped: {profile_data['name']}\")\n",
    "        time.sleep(random.uniform(2, 5))  # Random delay between requests\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "\n",
    "# Create a DataFrame from scraped data\n",
    "result_df = pd.DataFrame(scraped_data)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "result_df.to_csv('scraped_scholar_profiles.csv', index=False)\n",
    "print(\"Scraping completed. Results saved to 'scraped_scholar_profiles.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6SjzcBHrSUti",
    "outputId": "ad8ce31a-7f98-4025-e92a-522994930d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped research interests for: Martin Farach-Colton\n",
      "Scraped research interests for: Boris Aronov\n",
      "Scraped research interests for: Juan P Bello\n",
      "Scraped research interests for: Aaron Bernstein\n",
      "Scraped research interests for: Emily Black\n",
      "Scraped research interests for: Justin Cappos\n",
      "Scraped research interests for: Yi-Jen Chiang\n",
      "Scraped research interests for: Rumi Chunara\n",
      "Scraped research interests for: Peter DePasquale\n",
      "Scraped research interests for: Ratan Dey\n",
      "Scraped research interests for: Brendan Dolan-Gavitt\n",
      "Scraped research interests for: Juliana Freire\n",
      "Scraped research interests for: Rachel Greenstadt\n",
      "Scraped research interests for: Chinmay Hegde\n",
      "Scraped research interests for: Lisa Hellerstein\n",
      "Scraped research interests for: Robert Krueger\n",
      "Scraped research interests for: Damon McCoy\n",
      "Scraped research interests for: Nasir Memon\n",
      "Scraped research interests for: Christopher Musco\n",
      "Scraped research interests for: Darryl Reeves\n",
      "Scraped research interests for: Gustavo Sandoval\n",
      "Scraped research interests for: Claudio T. Silva\n",
      "Scraped research interests for: John Sterling\n",
      "Scraped research interests for: Julia Stoyanovich\n",
      "Scraped research interests for: Torsten Suel\n",
      "Scraped research interests for: Qi Sun\n",
      "Scraped research interests for: Julian Togelius\n",
      "Scraped research interests for: Paul Torrens\n",
      "Scraped research interests for: Erdem Varol\n",
      "Scraped research interests for: Edward K Wong\n",
      "Scraped research interests for: Jay Chen\n",
      "Scraped research interests for: Paweł Korus\n",
      "Scraped research interests for: Niall L. Williams\n",
      "Scraped research interests for: Dishita G Turakhia\n",
      "Scraped research interests for: Nairen Cao\n",
      "Scraped research interests for: Eugene Vinitsky\n",
      "Scraped research interests for: Brandon Reagen\n",
      "Scraped research interests for: Oded Nov\n",
      "Scraped research interests for: Daniel B. Neill\n",
      "Scraped research interests for: Ramesh Karri  కర్రి రమేష్\n",
      "Scraped research interests for: Danny Yuxing Huang\n",
      "Scraped research interests for: Siddharth Garg\n",
      "Scraped research interests for: Chen Feng (冯晨)\n",
      "Scraped research interests for: Semiha Ergan\n",
      "Scraping completed. Results saved to 'research_interests.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Read the CSV file with Google Scholar links\n",
    "df = pd.read_csv('/content/NYU_tandon_professors_links.csv', header=None, names=['url'])\n",
    "\n",
    "# Function to scrape research interests\n",
    "def scrape_research_interests(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    name = soup.find('div', id='gsc_prf_in').text if soup.find('div', id='gsc_prf_in') else 'N/A'\n",
    "\n",
    "    interests = soup.find('div', id='gsc_prf_int')\n",
    "    if interests:\n",
    "        interests = [i.text for i in interests.find_all('a')]\n",
    "    else:\n",
    "        interests = []\n",
    "\n",
    "    return {\n",
    "        'name': name,\n",
    "        'research_interests': interests,\n",
    "        'url': url\n",
    "    }\n",
    "\n",
    "# List to store scraped data\n",
    "scraped_data = []\n",
    "\n",
    "# Scrape each profile\n",
    "for url in df['url']:\n",
    "    try:\n",
    "        profile_data = scrape_research_interests(url)\n",
    "        scraped_data.append(profile_data)\n",
    "        print(f\"Scraped research interests for: {profile_data['name']}\")\n",
    "        time.sleep(random.uniform(2, 5))  # Random delay between requests\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "\n",
    "# Create a DataFrame from scraped data\n",
    "result_df = pd.DataFrame(scraped_data)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "result_df.to_csv('research_interests.csv', index=False)\n",
    "print(\"Scraping completed. Results saved to 'research_interests.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65RxDeabTF8Z",
    "outputId": "10c2bc5a-f61d-4e17-fed2-afcf34541c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped and saved data for: Martin Farach-Colton\n",
      "Scraped and saved data for: Boris Aronov\n",
      "Scraped and saved data for: Juan P Bello\n",
      "Scraped and saved data for: Aaron Bernstein\n",
      "Scraped and saved data for: Emily Black\n",
      "Scraped and saved data for: Justin Cappos\n",
      "Scraped and saved data for: Yi-Jen Chiang\n",
      "Scraped and saved data for: Rumi Chunara\n",
      "Scraped and saved data for: Peter DePasquale\n",
      "Scraped and saved data for: Ratan Dey\n",
      "Scraped and saved data for: Brendan Dolan-Gavitt\n",
      "Scraped and saved data for: Juliana Freire\n",
      "Scraped and saved data for: Rachel Greenstadt\n",
      "Scraped and saved data for: Chinmay Hegde\n",
      "Scraped and saved data for: Lisa Hellerstein\n",
      "Scraped and saved data for: Robert Krueger\n",
      "Scraped and saved data for: Damon McCoy\n",
      "Scraped and saved data for: Nasir Memon\n",
      "Scraped and saved data for: Christopher Musco\n",
      "Scraped and saved data for: Darryl Reeves\n",
      "Scraped and saved data for: Gustavo Sandoval\n",
      "Scraped and saved data for: Claudio T. Silva\n",
      "Scraped and saved data for: John Sterling\n",
      "Scraped and saved data for: Julia Stoyanovich\n",
      "Scraped and saved data for: Torsten Suel\n",
      "Scraped and saved data for: Qi Sun\n",
      "Scraped and saved data for: Julian Togelius\n",
      "Scraped and saved data for: Paul Torrens\n",
      "Scraped and saved data for: Erdem Varol\n",
      "Scraped and saved data for: Edward K Wong\n",
      "Scraped and saved data for: Jay Chen\n",
      "Scraped and saved data for: Paweł Korus\n",
      "Scraped and saved data for: Niall L. Williams\n",
      "Scraped and saved data for: Dishita G Turakhia\n",
      "Scraped and saved data for: Nairen Cao\n",
      "Scraped and saved data for: Eugene Vinitsky\n",
      "Scraped and saved data for: Brandon Reagen\n",
      "Scraped and saved data for: Oded Nov\n",
      "Scraped and saved data for: Daniel B. Neill\n",
      "Scraped and saved data for: Ramesh Karri  కర్రి రమేష్\n",
      "Scraped and saved data for: Danny Yuxing Huang\n",
      "Scraped and saved data for: Siddharth Garg\n",
      "Scraped and saved data for: Chen Feng (冯晨)\n",
      "Scraped and saved data for: Semiha Ergan\n",
      "Scraping completed. Individual professor data saved in 'professor_data' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Read the CSV file with Google Scholar links\n",
    "df = pd.read_csv('/content/NYU_tandon_professors_links.csv', header=None, names=['url'])\n",
    "\n",
    "# Function to scrape profile data\n",
    "def scrape_profile(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    name = soup.find('div', id='gsc_prf_in').text if soup.find('div', id='gsc_prf_in') else 'N/A'\n",
    "    affiliation = soup.find('div', class_='gsc_prf_il').text if soup.find('div', class_='gsc_prf_il') else 'N/A'\n",
    "\n",
    "    citations = soup.find('td', class_='gsc_rsb_std')\n",
    "    total_citations = citations.text if citations else 'N/A'\n",
    "\n",
    "    h_index = soup.find_all('td', class_='gsc_rsb_std')[2]\n",
    "    h_index = h_index.text if h_index else 'N/A'\n",
    "\n",
    "    interests = soup.find('div', id='gsc_prf_int')\n",
    "    if interests:\n",
    "        interests = [i.text for i in interests.find_all('a')]\n",
    "    else:\n",
    "        interests = []\n",
    "\n",
    "    return {\n",
    "        'name': name,\n",
    "        'affiliation': affiliation,\n",
    "        'total_citations': total_citations,\n",
    "        'h_index': h_index,\n",
    "        'research_interests': interests,\n",
    "        'url': url\n",
    "    }\n",
    "\n",
    "# Create a directory to store individual files\n",
    "os.makedirs('professor_data', exist_ok=True)\n",
    "\n",
    "# Scrape each profile and save to individual files\n",
    "for url in df['url']:\n",
    "    try:\n",
    "        profile_data = scrape_profile(url)\n",
    "\n",
    "        # Create a filename based on the professor's name\n",
    "        filename = f\"professor_data/{profile_data['name'].replace(' ', '_')}.json\"\n",
    "\n",
    "        # Save the data to a JSON file\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(profile_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Scraped and saved data for: {profile_data['name']}\")\n",
    "        time.sleep(random.uniform(2, 5))  # Random delay between requests\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "\n",
    "print(\"Scraping completed. Individual professor data saved in 'professor_data' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gR5TCit0USU3",
    "outputId": "c6b97f78-c622-4490-de7e-f596af430d61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped and saved data for: Martin Farach-Colton\n",
      "Scraped and saved data for: Boris Aronov\n",
      "Scraped and saved data for: Juan P Bello\n",
      "Scraped and saved data for: Aaron Bernstein\n",
      "Scraped and saved data for: Emily Black\n",
      "Scraped and saved data for: Justin Cappos\n",
      "Scraped and saved data for: Yi-Jen Chiang\n",
      "Scraped and saved data for: Rumi Chunara\n",
      "Scraped and saved data for: Peter DePasquale\n",
      "Scraped and saved data for: Ratan Dey\n",
      "Scraped and saved data for: Brendan Dolan-Gavitt\n",
      "Error scraping https://scholar.google.com/citations?user=sSzAlq0AAAAJ&hl=en: 'NoneType' object has no attribute 'text'\n",
      "Scraped and saved data for: Rachel Greenstadt\n",
      "Scraped and saved data for: Chinmay Hegde\n",
      "Scraped and saved data for: Lisa Hellerstein\n",
      "Scraped and saved data for: Robert Krueger\n",
      "Scraped and saved data for: Damon McCoy\n",
      "Scraped and saved data for: Nasir Memon\n",
      "Scraped and saved data for: Christopher Musco\n",
      "Scraped and saved data for: Darryl Reeves\n",
      "Scraped and saved data for: Gustavo Sandoval\n",
      "Scraped and saved data for: Claudio T. Silva\n",
      "Scraped and saved data for: John Sterling\n",
      "Scraped and saved data for: Julia Stoyanovich\n",
      "Scraped and saved data for: Torsten Suel\n",
      "Scraped and saved data for: Qi Sun\n",
      "Scraped and saved data for: Julian Togelius\n",
      "Scraped and saved data for: Paul Torrens\n",
      "Scraped and saved data for: Erdem Varol\n",
      "Scraped and saved data for: Edward K Wong\n",
      "Scraped and saved data for: Jay Chen\n",
      "Scraped and saved data for: Paweł Korus\n",
      "Scraped and saved data for: Niall L. Williams\n",
      "Scraped and saved data for: Dishita G Turakhia\n",
      "Scraped and saved data for: Nairen Cao\n",
      "Scraped and saved data for: Eugene Vinitsky\n",
      "Scraped and saved data for: Brandon Reagen\n",
      "Scraped and saved data for: Oded Nov\n",
      "Scraped and saved data for: Daniel B. Neill\n",
      "Scraped and saved data for: Ramesh Karri  కర్రి రమేష్\n",
      "Scraped and saved data for: Danny Yuxing Huang\n",
      "Scraped and saved data for: Siddharth Garg\n",
      "Scraped and saved data for: Chen Feng (冯晨)\n",
      "Scraped and saved data for: Semiha Ergan\n",
      "Scraping completed. Individual professor data saved in 'professor_data' directory.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Read the CSV file with Google Scholar links\n",
    "df = pd.read_csv('/content/NYU_tandon_professors_links.csv', header=None, names=['url'])\n",
    "\n",
    "# Function to scrape profile data and research papers\n",
    "def scrape_profile_and_papers(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    name = soup.find('div', id='gsc_prf_in').text if soup.find('div', id='gsc_prf_in') else 'N/A'\n",
    "    affiliation = soup.find('div', class_='gsc_prf_il').text if soup.find('div', class_='gsc_prf_il') else 'N/A'\n",
    "\n",
    "    # Scrape research papers\n",
    "    papers = []\n",
    "    paper_elements = soup.find_all('tr', class_='gsc_a_tr')\n",
    "    for paper in paper_elements:\n",
    "        title = paper.find('a', class_='gsc_a_at').text\n",
    "        authors = paper.find('div', class_='gs_gray').text\n",
    "        publication = paper.find_all('div', class_='gs_gray')[1].text\n",
    "        year = paper.find('span', class_='gsc_a_h gsc_a_hc gs_ibl').text\n",
    "        citations = paper.find('a', class_='gsc_a_ac gs_ibl').text\n",
    "\n",
    "        papers.append({\n",
    "            'title': title,\n",
    "            'authors': authors,\n",
    "            'publication': publication,\n",
    "            'year': year,\n",
    "            'citations': citations\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        'name': name,\n",
    "        'affiliation': affiliation,\n",
    "        'url': url,\n",
    "        'papers': papers\n",
    "    }\n",
    "\n",
    "# Create a directory to store individual files\n",
    "os.makedirs('professor_data', exist_ok=True)\n",
    "\n",
    "# Scrape each profile and save to individual files\n",
    "for url in df['url']:\n",
    "    try:\n",
    "        profile_data = scrape_profile_and_papers(url)\n",
    "\n",
    "        # Create a filename based on the professor's name\n",
    "        filename = f\"professor_data/{profile_data['name'].replace(' ', '_')}.json\"\n",
    "\n",
    "        # Save the data to a JSON file\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(profile_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Scraped and saved data for: {profile_data['name']}\")\n",
    "        time.sleep(random.uniform(2, 5))  # Random delay between requests\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {str(e)}\")\n",
    "\n",
    "print(\"Scraping completed. Individual professor data saved in 'professor_data' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "-E64WCicfp_V",
    "outputId": "db204236-7df0-4a72-b185-eddd96148b17"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_43a0dd16-6b41-440a-b13e-04dcb1f1d664\", \"content_folder.zip\", 7176617)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Define the source folder and the name of the zip file\n",
    "source_folder = '/content'\n",
    "zip_file_name = 'content_folder.zip'\n",
    "\n",
    "# Create a zip archive of the /content folder\n",
    "shutil.make_archive(zip_file_name.replace('.zip', ''), 'zip', source_folder)\n",
    "\n",
    "# Download the zip file\n",
    "files.download(zip_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Jq3bVADgKOO",
    "outputId": "849f086b-24ac-4e5d-d4eb-6a0df5b2c5df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in /home/ubuntu/.local/lib/python3.10/site-packages (4.9.1)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /home/ubuntu/.local/lib/python3.10/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /home/ubuntu/.local/lib/python3.10/site-packages (from selenium) (2024.7.4)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /home/ubuntu/.local/lib/python3.10/site-packages (from selenium) (1.26.20)\n",
      "Requirement already satisfied: trio~=0.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/ubuntu/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /home/ubuntu/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: outcome in /home/ubuntu/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/ubuntu/.local/lib/python3.10/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Reading package lists... Done\n",
      "E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n",
      "E: Unable to lock directory /var/lib/apt/lists/\n",
      "W: Problem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied)\n",
      "W: Problem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied)\n",
      "\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mUnable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages first\n",
    "!pip install selenium\n",
    "!apt-get update\n",
    "!apt install chromium-chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUX3g0TzgYHn",
    "outputId": "312d467e-eff4-418c-81b0-125903abb317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing professor 1/44\n",
      "Processing professor 2/44\n",
      "Processing professor 3/44\n",
      "Processing professor 4/44\n",
      "Processing professor 5/44\n",
      "Processing professor 6/44\n",
      "Processing professor 7/44\n",
      "Processing professor 8/44\n",
      "Processing professor 9/44\n",
      "Processing professor 10/44\n",
      "Processing professor 11/44\n",
      "Processing professor 12/44\n",
      "Processing professor 13/44\n",
      "Processing professor 14/44\n",
      "Processing professor 15/44\n",
      "Processing professor 16/44\n",
      "Processing professor 17/44\n",
      "Processing professor 18/44\n",
      "Processing professor 19/44\n",
      "Processing professor 20/44\n",
      "Processing professor 21/44\n",
      "Processing professor 22/44\n",
      "Processing professor 23/44\n",
      "Processing professor 24/44\n",
      "Processing professor 25/44\n",
      "Processing professor 26/44\n",
      "Processing professor 27/44\n",
      "Processing professor 28/44\n",
      "Processing professor 29/44\n",
      "Processing professor 30/44\n",
      "Processing professor 31/44\n",
      "Processing professor 32/44\n",
      "Processing professor 33/44\n",
      "Processing professor 34/44\n",
      "Processing professor 35/44\n",
      "Processing professor 36/44\n",
      "Processing professor 37/44\n",
      "Processing professor 38/44\n",
      "Processing professor 39/44\n",
      "Processing professor 40/44\n",
      "Processing professor 41/44\n",
      "Processing professor 42/44\n",
      "Processing professor 43/44\n",
      "Processing professor 44/44\n",
      "Data saved to /content/professor_papers_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def setup_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    return webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def scrape_paper_data(url, driver):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(random.uniform(2, 3))\n",
    "\n",
    "        # Get professor name\n",
    "        name = driver.find_element(By.ID, 'gsc_prf_in').text\n",
    "\n",
    "        # Get research interests\n",
    "        interests = []\n",
    "        interest_elements = driver.find_elements(By.CLASS_NAME, 'gsc_prf_inta')\n",
    "        for element in interest_elements:\n",
    "            interests.append(element.text)\n",
    "\n",
    "        # Get papers\n",
    "        papers = []\n",
    "        paper_elements = driver.find_elements(By.CLASS_NAME, 'gsc_a_tr')\n",
    "\n",
    "        for paper in paper_elements:\n",
    "            try:\n",
    "                title_element = paper.find_element(By.CLASS_NAME, 'gsc_a_at')\n",
    "                title = title_element.text\n",
    "                paper_url = title_element.get_attribute('href')\n",
    "\n",
    "                # Get paper description if available\n",
    "                description = \"Description not available\"\n",
    "                if paper_url:\n",
    "                    driver.get(paper_url)\n",
    "                    time.sleep(random.uniform(1, 2))\n",
    "                    desc_elements = driver.find_elements(By.CLASS_NAME, 'gsh_csp')\n",
    "                    if desc_elements:\n",
    "                        description = desc_elements[0].text\n",
    "\n",
    "                papers.append({\n",
    "                    'title': title,\n",
    "                    'description': description\n",
    "                })\n",
    "\n",
    "                # Go back to profile page\n",
    "                driver.back()\n",
    "                time.sleep(random.uniform(1, 2))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing paper: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        return {\n",
    "            'name': name,\n",
    "            'research_interests': interests,\n",
    "            'papers': papers\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping profile: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Read professor links\n",
    "    df = pd.read_csv('NYU_tandon_professors_links.csv', header=None, names=['url'])\n",
    "\n",
    "    # Setup output file\n",
    "    output_file = 'professor_papers_data.csv'\n",
    "\n",
    "    # Initialize webdriver\n",
    "    driver = setup_driver()\n",
    "\n",
    "    # Process each professor\n",
    "    all_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        print(f\"Processing professor {index + 1}/{len(df)}\")\n",
    "        data = scrape_paper_data(row['url'], driver)\n",
    "        if data:\n",
    "            for paper in data['papers']:\n",
    "                all_data.append({\n",
    "                    'Professor Name': data['name'],\n",
    "                    'Professor Link': row['url'],\n",
    "                    'Research Interests': ', '.join(data['research_interests']),\n",
    "                    'Paper Name': paper['title'],\n",
    "                    'Description': paper['description']\n",
    "                })\n",
    "        time.sleep(random.uniform(3, 5))\n",
    "\n",
    "    # Save to CSV\n",
    "    pd.DataFrame(all_data).to_csv(output_file, index=False)\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "\n",
    "    # Close driver\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
